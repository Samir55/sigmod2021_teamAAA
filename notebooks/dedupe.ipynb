{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "import re\n",
    "import spacy\n",
    "import dedupe\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    num = float(num)\n",
    "    if num % 1 == 0:\n",
    "        return int(num)\n",
    "    else:\n",
    "        return num\n",
    "def fill_nulls_with_none(df):\n",
    "    \"\"\" Fills nulls in a dataframe with None.\n",
    "        This is required for the Dedupe package to work properly.\n",
    "\n",
    "        Input: - dataframe with nulls as NaN\n",
    "\n",
    "        Output: - new dataframe with nulls as None\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        new_df[col] = new_df[col].where(new_df[col].notnull(), None)\n",
    "    return new_df\n",
    "\n",
    "def convert_numbers_to_strings(df, cols_to_convert, remove_point_zero=True):\n",
    "    \"\"\" Convert number types to strings in a dataframe.\n",
    "        This is convoluted as need to keep NoneTypes as NoneTypes for what comes next!\n",
    "\n",
    "        Inputs: - df -> dataframe to convert number types\n",
    "                - cols_to_convert -> list of columns to convert\n",
    "                - remove_point_zero -> bool to say whether you want '.0' removed from number\n",
    "\n",
    "        Outputs: - dataframe with converted number types\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col in cols_to_convert:\n",
    "        if remove_point_zero:\n",
    "            new_df[col] = new_df[col].apply(lambda x: str(x).replace('.0','')\\\n",
    "                                            if not isinstance(x, type(None)) else x)\n",
    "        else:\n",
    "            new_df[col] = new_df[col].apply(lambda x: str(x)\\\n",
    "                                            if not isinstance(x, type(None)) else x)\n",
    "    return new_df\n",
    "\n",
    "extra_brands = set(pd.read_csv('laptops.csv').Company.str.lower().unique())\n",
    "screen_sizes = set(pd.read_csv('laptops.csv').Inches)\n",
    "screen_sizes = [str(format_number(str(s).lower())) for s in screen_sizes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# From Jerry's code. Might not be needed:\n",
    "title_remove_words = [\"price\", \"comparison\", \"at\", \"buy.net\", \"amazon.com\", \":\",\n",
    "\"computers\", \"&\", \"accessories\", \"laptop\", \"vology\", \"tigerdirect\", \".com\", \"ultraportable\", \"cool\",\n",
    "\"audiophile\", \"wireless\", \"bluetooth\", \"speaker\", \"portable\", \"with\", \"built-in\", \"microphone\", \"and\",\n",
    "\"micro\", \"sd\",\"card\",\"slot\", \"-\", \"(\", \")\", \"high\", \"performance\", \"new\", \"core\", \"high\", \"end\", \"bes\",\n",
    "\"audio\", \"nx.m8eaa.007\", \"/\", \"notebook\", \"pc\", '\"', \"brand\", \"new\", \"hewlett-packard\"]\n",
    "\n",
    "def preprocess_laptop_dataset(df):\n",
    "    # Alpha numeric\n",
    "    irrelevant_regex = re.compile(r\"[^a-z0-9,.\\-\\s]\")\n",
    "    multispace_regex = re.compile(r'\\s\\s+') # Why it doesn't work\n",
    "    df.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column == 'instance_id':\n",
    "            continue\n",
    "        df[column] = df[column].str.lower().str.replace(irrelevant_regex, ' ').str.replace(multispace_regex, ' ')\n",
    "\n",
    "\n",
    "    def tokenize_new_tile(record):\n",
    "        return [w.text for w in sp(record['new_title'])]\n",
    "\n",
    "    df['new_title'] = df.title\n",
    "    irrelevant_regex = re.compile(r\"[^a-z0-9.\\s]\")\n",
    "    multispace_regex = re.compile(r'\\s\\s+') # Why it doesn't work\n",
    "    df['new_title'] = df.new_title.str.lower().str.replace(irrelevant_regex, '').str.replace(multispace_regex, ' ')\n",
    "    df['new_title_tokens'] =  df.apply(tokenize_new_tile, axis=1)\n",
    "\n",
    "    # Brand assignment\n",
    "    all_brands = set()\n",
    "\n",
    "    all_brands.update(extra_brands)\n",
    "\n",
    "    def assign_brand(record):\n",
    "        # Search in brand first\n",
    "        if record['brand'] in all_brands:\n",
    "            return record['brand']\n",
    "        # then in the title\n",
    "        for el in all_brands:\n",
    "            if el in record['title']:\n",
    "                return el\n",
    "        return \"NNN\"\n",
    "\n",
    "    df['brand'] = df.apply(assign_brand, axis=1)\n",
    "\n",
    "    # cpu brand\n",
    "    intel=['intel', 'i3', 'i5', 'i7'] #Needed because not all entries have intel\n",
    "    def assign_cpu_brand(record):\n",
    "        # Search in brand first\n",
    "        for blue in intel:\n",
    "            if blue in str(record['cpu_brand']) or blue in str(record['title']) or \\\n",
    "                    blue in str(record['cpu_model']) or blue in str(record['cpu_type']):\n",
    "                return 'intel'\n",
    "        return 'amd'\n",
    "\n",
    "    df['cpu_brand'] = df.apply(assign_cpu_brand, axis=1)\n",
    "\n",
    "    def assign_screen_size(record):\n",
    "        brand_tokens = record['new_title_tokens']\n",
    "        arr = []\n",
    "        for t in brand_tokens:\n",
    "            s = t.replace('inch', '')\n",
    "            s = s.replace('in', '')\n",
    "            arr.append(s)\n",
    "\n",
    "        for sc in screen_sizes:\n",
    "            if str(sc) in arr:\n",
    "                return str(sc)\n",
    "\n",
    "        else:\n",
    "            return str(15.6) # Some relaxation\n",
    "    df['screen_size'] = df.apply(assign_screen_size, axis=1)\n",
    "\n",
    "\n",
    "    # # ram capacity\n",
    "    # def assign_ram_capacity(record):\n",
    "    #     s = str(record['ram_capacity']).replace(' ', '')\n",
    "    #     possible_vals = ['2gb', '4gb', '6gb', '8gb', '10gb', '12gb', '16gb',\n",
    "    #                      '32gb', '64gb', '128gb', '256gb', '512gb', '2', '4',\n",
    "    #                      '6', '8', '10', '12', '16', '32', '64', '128']\n",
    "    #     for val in possible_vals:\n",
    "    #         if val in s:\n",
    "    #             return int(val.replace('gb', ''))\n",
    "    #\n",
    "    #     s = str(record['title']).replace(' ', '')  # This will be wrong, please change\n",
    "    #     possible_vals = ['2gb', '4gb', '6gb', '8gb', '10gb', '12gb', '16gb',\n",
    "    #                      '32gb', '64gb', '128gb']\n",
    "    #     for val in possible_vals:\n",
    "    #         if val in s:\n",
    "    #             return int(val.replace('gb', ''))\n",
    "    #\n",
    "    #     return 0\n",
    "        #new ram capacity\n",
    "    def assign_ram_capacity(record):\n",
    "        s = str(record['ram_capacity'])\n",
    "        t = str(record['title'])\n",
    "        regex = re.compile(r\"(\\d{1,3})\\s?([gm]b)\") # rare chance of encountering MB as an error\n",
    "        m = None\n",
    "        #ram_c = df['ram_capacity'].str.extract(regex)\n",
    "        #title_ram = df['title'].str.extract(regex)\n",
    "        if s:\n",
    "            m=re.search(regex, s)\n",
    "        if m is None:\n",
    "            m=re.search(regex, t)\n",
    "        if m is None:\n",
    "            return None\n",
    "        else:\n",
    "            m= m.group()\n",
    "            return re.sub(r'([gm]b)', \"\", m) # remove MB and GB\n",
    "\n",
    "    df['ram_capacity'] = df.apply(assign_ram_capacity, axis=1)\n",
    "\n",
    "    def assign_ram_type(record):\n",
    "        m = None\n",
    "        if \"ddr3\" in record['ram_type']:\n",
    "            return \"ddr3\"\n",
    "\n",
    "    df['ram_type'] = df.apply(assign_ram_type, axis=1)\n",
    "\n",
    "\n",
    "    def assign_hdd_capacity(record):\n",
    "        s = str(record['hdd_capacity']).replace(' ', '')\n",
    "        s2 = str(record['title'].replace(' ', ''))\n",
    "\n",
    "        if 'ssd' in s:\n",
    "            return 0\n",
    "\n",
    "        if re.search(\"\\d{3,4}gb\", s):\n",
    "            return int(re.findall(\"\\d{3,4}gb\", s)[0][:-2])\n",
    "        if re.search(\"\\dtb\", s):\n",
    "            return int(re.findall(\"\\dtb\", s)[0][:-2] + '000')\n",
    "        if re.search(\"\\d{3,4}gbhdd\", s2):\n",
    "            return int(re.findall(\"\\d{3,4}gbhdd\", s2)[0][:-5])\n",
    "        if re.search(\"hdd\\d{3,4}gb\", s2):\n",
    "            return int(re.findall(\"hdd\\d{3,4}gb\", s2)[0][3:-2])\n",
    "        if re.search(\"hdd\\dtb\", s2):\n",
    "            return int(re.findall(\"hdd\\dtb\", s2)[0][3:4] + '000')\n",
    "        if re.search(\"\\dtbhdd\", s2):\n",
    "            return int(re.findall(\"\\dtbhdd\", s2)[0][0] + '000')\n",
    "        return 0\n",
    "    df['hdd_capacity'] = df.apply(assign_hdd_capacity, axis=1)\n",
    "\n",
    "    def assign_ssd_capacity(record):\n",
    "        s = str(record['ssd_capacity']).replace(' ', '')\n",
    "        s2 = str(record['title'].replace(' ', ''))\n",
    "\n",
    "\n",
    "        if re.search(\"\\d{3,4}gb\", s):\n",
    "            return int(re.findall(\"\\d{3,4}gb\", s)[0][:-2])\n",
    "        if re.search(\"\\dtb\", s):\n",
    "            return int(re.findall(\"\\dtb\", s)[0][:-2] + '000')\n",
    "        if re.search(\"\\d{3,4}gbssd\", s2):\n",
    "            return int(re.findall(\"\\d{3,4}gbssd\", s2)[0][:-5])\n",
    "        if re.search(\"ssd\\d{3,4}gb\", s2):\n",
    "            return int(re.findall(\"ssd\\d{3,4}gb\", s2)[0][3:-2])\n",
    "        if re.search(\"ssd\\dtb\", s2):\n",
    "            return int(re.findall(\"ssd\\dtb\", s2)[0][3:4] + '000')\n",
    "        if re.search(\"\\dtbssd\", s2):\n",
    "            return int(re.findall(\"\\dtbssd\", s2)[0][0] + '000')\n",
    "        return 0\n",
    "\n",
    "    df['ssd_capacity'] = df.apply(assign_ssd_capacity, axis=1)\n",
    "\n",
    "    def assign_laptop_model(record):\n",
    "        brand_tokens = record['new_title_tokens']\n",
    "        try:\n",
    "            brand_index = brand_tokens.index(str(record['brand']))\n",
    "            finish_index = brand_index + 2\n",
    "            should_break = False\n",
    "            for i in range(2 + brand_index, 5 + brand_index, 1):\n",
    "                for sc in screen_sizes:\n",
    "                    if sc in brand_tokens[i]:\n",
    "                        should_break = True\n",
    "                        break\n",
    "                if should_break:\n",
    "                    if finish_index == i:\n",
    "                        finish_index -=1\n",
    "                    break\n",
    "                if not (brand_tokens[i].isalpha()):\n",
    "                    finish_index = i\n",
    "                else:\n",
    "                    break\n",
    "        except Exception:\n",
    "            brand_index = -1\n",
    "\n",
    "        if brand_index == -1:\n",
    "            return None\n",
    "\n",
    "        return ' '.join(brand_tokens[brand_index+1:finish_index+1])\n",
    "\n",
    "    # Intermediate column\n",
    "    df['model'] = df.apply(assign_laptop_model, axis=1)\n",
    "\n",
    "    def assign_model_name(record): # laptop Line\n",
    "        #print(record['model'].split())\n",
    "        if record['model'] is None:\n",
    "            return None\n",
    "        ans = record['model'].split(\" \")[0]\n",
    "        if ans.isalpha():\n",
    "            return ans\n",
    "        return None\n",
    "\n",
    "    df['model_name'] = df.apply(assign_model_name, axis=1)\n",
    "\n",
    "    def assign_model_number(record):\n",
    "        '''\n",
    "        if \"x230\" in org_title and \"3435\" in org_title:\n",
    "                        mod_item[\"model\"] = \"3435\"\n",
    "\n",
    "                    if \"hp\" in org_title:\n",
    "                        #regex for specific HP laptops\n",
    "                        hp_li = hp_new_model.findall(org_title)\n",
    "                        if len(hp_li) > 0:\n",
    "                            mod_item[\"model\"] = \" \".join(hp_li[0].replace(\"-\",\"\").replace(\" \",\"\").split())\n",
    "\n",
    "                    if \"hp\" in org_title and \"revolve\" in org_title and \"810\" in org_title:\n",
    "                        mod_item[\"model\"] = \"revolve 810 \"\n",
    "                        if \"g1\" in org_title.lower():\n",
    "                            mod_item[\"model\"] += \"g1\"\n",
    "                        elif \"g2\" in org_title.lower():\n",
    "                            mod_item[\"model\"] += \"g2\"\n",
    "\n",
    "                    if \"hp\" in org_title and \"compaq\" in org_title and \"nc6400\" in org_title:\n",
    "                        mod_item[\"model\"] = \"nc6400\"\n",
    "\n",
    "                    if \"lenovo\" in org_title or \"thinkpad\" in org_title:\n",
    "                        tp_li = lenovo_thinkpad_model.findall(org_title)\n",
    "                        if len(tp_li) > 0:\n",
    "                            mod_item[\"model\"] = \" \".join(tp_li[0].split())\n",
    "        '''\n",
    "        return \"232\";\n",
    "\n",
    "    df['model_number'] = df.apply(assign_model_number)\n",
    "\n",
    "    df = fill_nulls_with_none(df)\n",
    "    df = convert_numbers_to_strings(df, ['screen_size'])\n",
    "    # Unit stand. in weight\n",
    "    def assign_weight(record): #TO DO: Convert kg to lb if needed\n",
    "        regex=re.compile('.?(\\d{1,2}\\.\\d{1,2})\\s?[lpk]')\n",
    "        s = record['weight']\n",
    "        m = None\n",
    "        if s:\n",
    "            m = re.search(regex, s)\n",
    "        if m is None:\n",
    "            m = re.search(regex, record['title'])\n",
    "        if m is None:\n",
    "            return None\n",
    "        else:\n",
    "            m = m.group()\n",
    "            return re.sub(r\"\\s?[lpk]\", \"\", m)\n",
    "\n",
    "    df['weight'] = df.apply(assign_weight, axis=1)\n",
    "\n",
    "    def assign_cpu_type(record):\n",
    "    # Find the cpu type\n",
    "        cpu_list = [\"i5\", \"i3\", \"i7\", \"atom\",\n",
    "                    \"pentium\", \"celeron\", \"a-series\",\n",
    "                    \"e-series\", \"aseries\", \"eseries\",\n",
    "                    \"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\", \"a7\", \"a8\", \"a9\"]\n",
    "\n",
    "        for cpu in cpu_list:\n",
    "            if record['cpu_type'] is not None and cpu in record['cpu_type']:\n",
    "                return cpu\n",
    "            if cpu in record['title']:\n",
    "                return cpu\n",
    "            if record['cpu_model'] is not None and cpu in record['cpu_model']:\n",
    "                return cpu\n",
    "            if record['cpu_frequency'] is not None and  cpu in record['cpu_frequency']:\n",
    "                return cpu\n",
    "\n",
    "            if re.search(\"e-[0-9]{3}\", record['title']):\n",
    "                return re.findall(\"e-[0-9]{3}\", record['title'])[0]\n",
    "\n",
    "            if record['cpu_model'] is not None and re.search(\"e-[0-9]{3}\", record['cpu_model']):\n",
    "                return re.findall(\"e-[0-9]{3}\", record['cpu_model'])[0]\n",
    "\n",
    "    df['cpu_type'] = df.apply(assign_cpu_type, axis=1)\n",
    "\n",
    "    #TO DO: there are laptops called E1-572 and cpus called E1-2100 or E-300\n",
    "    def assign_cpu_model(record):\n",
    "        model=record['cpu_model']\n",
    "        regex=re.compile(r\"-?\\d{1,4}([mu])\") #For intel cpus\n",
    "        regex2=re.compile(r\"[ea]\\d?-\\d{1,4}[m]?\") #for amd A and E series. Needs detection after AMD tag in title\n",
    "        m=None\n",
    "        if record['cpu_brand']=='intel' and model is not None :\n",
    "            m = re.search(regex, model)\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'-', \"\", m)\n",
    "        if re.search(\"intel\", record['title']): # one case where laptop model is 50m and gets caught\n",
    "            m = re.search(regex, record['title'])\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'-', \"\", m)\n",
    "        if record['cpu_brand']=='amd' and model is not None:\n",
    "            m = re.search(regex2, model)\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'[ea]\\d?-', \"\", m)\n",
    "        if re.search(\"amd\", record['title']):\n",
    "            m = re.search(regex2, record['title'])\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'[ea]\\d?-', \"\", m)\n",
    "        if m is None:\n",
    "            return None\n",
    "\n",
    "    df['cpu_model'] = df.apply(assign_cpu_model, axis=1)\n",
    "\n",
    "    def assign_cpu_frequency(record):\n",
    "        s = record['cpu_frequency']\n",
    "        regex=re.compile(r\"\\d?.\\d{1,2}\\s?ghz\")\n",
    "        m = None\n",
    "        if s:\n",
    "            m=re.search(regex, s)\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'ghz', \"\", m)\n",
    "        if re.search(\"ghz\", record['title']):\n",
    "            m = re.search(regex, record['title'])\n",
    "            if m is not None:\n",
    "                m=m.group()\n",
    "                return re.sub(r'ghz', \"\", m)\n",
    "        if m is None:\n",
    "            return None\n",
    "    df['cpu_frequency'] = df.apply(assign_cpu_frequency, axis=1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nRedundant with previous functions\\ndef fill_nulls_with_none(df):\\n    \"\"\" Fills nulls in a dataframe with None.\\n        This is required for the Dedupe package to work properly.\\n        \\n        Input: - dataframe with nulls as NaN\\n        \\n        Output: - new dataframe with nulls as None\\n    \"\"\"\\n    new_df = df.copy()\\n    for col in df.columns:\\n        new_df[col] = new_df[col].where(new_df[col].notnull(), None)\\n    return new_df\\n\\ndef convert_numbers_to_strings(df, cols_to_convert, remove_point_zero=True):\\n    \"\"\" Convert number types to strings in a dataframe.\\n        This is convoluted as need to keep NoneTypes as NoneTypes for what comes next!\\n        \\n        Inputs: - df -> dataframe to convert number types\\n                - cols_to_convert -> list of columns to convert\\n                - remove_point_zero -> bool to say whether you want \\'.0\\' removed from number\\n        \\n        Outputs: - dataframe with converted number types\\n    \"\"\"\\n    new_df = df.copy()\\n    for col in cols_to_convert:\\n        if remove_point_zero:\\n            new_df[col] = new_df[col].apply(lambda x: str(x).replace(\\'.0\\',\\'\\')                                            if not isinstance(x, type(None)) else x)\\n        else:\\n            new_df[col] = new_df[col].apply(lambda x: str(x)                                            if not isinstance(x, type(None)) else x)\\n    return new_df\\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Redundant with previous functions\n",
    "def fill_nulls_with_none(df):\n",
    "    \"\"\" Fills nulls in a dataframe with None.\n",
    "        This is required for the Dedupe package to work properly.\n",
    "        \n",
    "        Input: - dataframe with nulls as NaN\n",
    "        \n",
    "        Output: - new dataframe with nulls as None\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        new_df[col] = new_df[col].where(new_df[col].notnull(), None)\n",
    "    return new_df\n",
    "\n",
    "def convert_numbers_to_strings(df, cols_to_convert, remove_point_zero=True):\n",
    "    \"\"\" Convert number types to strings in a dataframe.\n",
    "        This is convoluted as need to keep NoneTypes as NoneTypes for what comes next!\n",
    "        \n",
    "        Inputs: - df -> dataframe to convert number types\n",
    "                - cols_to_convert -> list of columns to convert\n",
    "                - remove_point_zero -> bool to say whether you want '.0' removed from number\n",
    "        \n",
    "        Outputs: - dataframe with converted number types\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col in cols_to_convert:\n",
    "        if remove_point_zero:\n",
    "            new_df[col] = new_df[col].apply(lambda x: str(x).replace('.0','')\\\n",
    "                                            if not isinstance(x, type(None)) else x)\n",
    "        else:\n",
    "            new_df[col] = new_df[col].apply(lambda x: str(x)\\\n",
    "                                            if not isinstance(x, type(None)) else x)\n",
    "    return new_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-8433c0f700e7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mx2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/sigmod/X2.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mx2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'instance_id'\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdrop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mx2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreprocess_laptop_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdeep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;31m#x2.ram_capacity[50:60]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-92e6b63754af>\u001B[0m in \u001B[0;36mpreprocess_laptop_dataset\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m    116\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;34m\"ddr3\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 118\u001B[1;33m     \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'ram_type'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0massign_ram_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\coding projects\\sigmod\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001B[0m\n\u001B[0;32m   7766\u001B[0m             \u001B[0mkwds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7767\u001B[0m         )\n\u001B[1;32m-> 7768\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7769\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7770\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapplymap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_action\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\coding projects\\sigmod\\venv\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mget_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    183\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_raw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 185\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    186\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapply_empty_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\coding projects\\sigmod\\venv\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    275\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 276\u001B[1;33m         \u001B[0mresults\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_series_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    277\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m         \u001B[1;31m# wrap results\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\coding projects\\sigmod\\venv\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    288\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseries_gen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    289\u001B[0m                 \u001B[1;31m# ignore SettingWithCopy here in case the user mutates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 290\u001B[1;33m                 \u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    291\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m                     \u001B[1;31m# If we have a view on v, we need to make a copy because\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-92e6b63754af>\u001B[0m in \u001B[0;36massign_ram_type\u001B[1;34m(record)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0massign_ram_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m         \u001B[0mm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 115\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[1;34m\"ddr3\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrecord\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'ram_type'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    116\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;34m\"ddr3\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "x2 = pd.read_csv(\"../data/sigmod/X2.csv\")\n",
    "x2.set_index('instance_id',  inplace=True, drop=False)\n",
    "x2 = preprocess_laptop_dataset(x2.copy(deep=True))\n",
    "#x2.ram_capacity[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = pd.read_csv(\"../data/sigmod/X3.csv\")\n",
    "x3.set_index('instance_id',  inplace=True, drop=False)\n",
    "x3 = preprocess_laptop_dataset(x3.copy(deep=True))\n",
    "# x3 = convert_numbers_to_strings(x3, ['ram_capacity', 'screen_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x2), len(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.index.intersection(x3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x23 = x3.append(x2)\n",
    "x23 = x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x23.columns, len(x23.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dedupe = x23[[\n",
    "    'instance_id',\n",
    "    'brand',\n",
    "    'cpu_brand',\n",
    "    'cpu_type',\n",
    "    'ram_capacity',\n",
    "    'hdd_capacity', \n",
    "    'ssd_capacity',\n",
    "    'title',\n",
    "    'screen_size',\n",
    "    'model']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dedupe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dedupe_dict = to_dedupe.to_dict(orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dedupe_dict['www.softwarecity.ca//737']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_dedupe_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(to_dedupe_dict, f)\n",
    "with open('to_dedupe_dict.pkl', 'rb') as f:\n",
    "    to_dedupe_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs for this are here: \n",
    "fields = [{'field' : 'brand', 'type' : 'Categorical', 'categories' : extra_brands},\n",
    "          \n",
    "          {'field' : 'cpu_brand', 'type': 'Categorical', 'categories' : ['amd', 'intel']}, \n",
    "          \n",
    "#           {'field' : 'cpu_model', 'type': 'String', 'has_missing' : True},\n",
    "          \n",
    "          {'field' : 'cpu_type', 'type': 'Exact', 'has_missing' : False},\n",
    "          \n",
    "          {'field' : 'ram_capacity', 'type': 'Price', 'has_missing' : False},\n",
    "          \n",
    "          {'field' : 'hdd_capacity', 'type': 'Price', 'has_missing' : False},\n",
    "          \n",
    "          {'field' : 'ssd_capacity', 'type': 'Price', 'has_missing' : False},\n",
    "          \n",
    "          {'field' : 'title', 'type': 'Text', 'has_missing' : False},\n",
    "          \n",
    "          {'field' : 'screen_size', 'type': 'Categorical', 'has_missing' : False, 'categories' : screen_sizes},\n",
    "          \n",
    "          {'field' : 'model', 'type': 'String', 'has_missing' : True},\n",
    "          \n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a bug later on that requires num_cores to be 1, but we can make use of\n",
    "# multi-threaded processes in the meantime\n",
    "deduper = dedupe.Dedupe(fields, num_cores=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = pd.read_csv('../data/sigmod/Y2.csv')\n",
    "y3 = pd.read_csv('../data/sigmod/Y3.csv')\n",
    "# y = y3.append(y2)\n",
    "y = y2\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {'match': [], 'distinct': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = y[y.label == 1].to_dict(orient='row')\n",
    "distinct = y[y.label == 0].to_dict(orient='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in match:\n",
    "    training_data['match'].append( ( to_dedupe_dict[m['left_instance_id']], to_dedupe_dict[m['right_instance_id']] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data['match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in distinct:\n",
    "    training_data['distinct'].append( ( to_dedupe_dict[d['left_instance_id']], to_dedupe_dict[d['right_instance_id']] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data['distinct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data['match'].extend(my_own_annotation['match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data['distinct'].extend(my_own_annotation['distinct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_laptop.json', 'w') as fout:\n",
    "    json.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'y_laptop.json'\n",
    "with open(training_file) as tf:\n",
    "    deduper.prepare_training(to_dedupe_dict, training_file=tf, sample_size=1500, blocked_proportion=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('starting active labeling...')\n",
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_laptop_augmented.json', 'w') as fout:\n",
    "    json.dump(deduper.training_pairs, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train(recall=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'trained_model_laptops.json'\n",
    "settings_file = 'trained_model_laptops_settings.json'\n",
    "with open(training_file, 'w') as tf:\n",
    "    deduper.write_training(tf)\n",
    "with open(settings_file, 'wb') as sf:\n",
    "    deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dupes = deduper.partition(to_dedupe_dict, 0.5)\n",
    "\n",
    "print('# duplicate sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership = {}\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        cluster_membership[record_id] = {\n",
    "            \"Cluster ID\": cluster_id,\n",
    "            \"confidence_score\": score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laptop_test(x):\n",
    "    x.set_index('instance_id',  inplace=True, drop=False)\n",
    "    x = preprocess_laptop_dataset(x.copy(deep=True))\n",
    "    \n",
    "    to_dedupe = x[[ 'instance_id',\n",
    "    'brand',\n",
    "    'cpu_brand',\n",
    "    'cpu_type',\n",
    "    'ram_capacity',\n",
    "    'hdd_capacity', \n",
    "    'ssd_capacity',\n",
    "    'title',\n",
    "    'screen_size',\n",
    "    'model']].copy()\n",
    "    \n",
    "    with open('trained_model_laptops_settings.json', 'rb') as fin:\n",
    "        dr = dedupe.StaticDedupe(fin)\n",
    "    to_dedupe_dict = to_dedupe.to_dict(orient = 'index')\n",
    "    \n",
    "    clustered_dupes = dr.partition(to_dedupe_dict, 0.5)\n",
    "\n",
    "    print('# duplicate sets', len(clustered_dupes))\n",
    "    \n",
    "    res = []\n",
    "    for el in clustered_dupes:\n",
    "        for i in range(len(el[0])):\n",
    "            for j in range(i+1, len(el[0])):\n",
    "                res.append((el[0][i], el[0][j]))\n",
    "    res_df =pd.DataFrame(res)            \n",
    "#     res_df.columns = ['left_instance_id', 'right_instance_id']\n",
    "    res_df.columns = ['left_instance_id', 'right_instance_id']\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pd.read_csv(\"../data/sigmod/X2.csv\")\n",
    "res = laptop_test(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = pd.read_csv(\"../data/sigmod/X3.csv\")\n",
    "res = laptop_test(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"output_x2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.data_model.predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}